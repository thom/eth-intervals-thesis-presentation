%==============================================================================
% presentation.tex
%==============================================================================


%==============================================================================
% Configuration
%==============================================================================

% Internationalisation
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% \usepackage[ngerman]{babel}

% Miscellaneous packages
\usepackage{url}
\usepackage{color,listings,paralist}
\usepackage{enumerate}
\usepackage{tabularx}
\usepackage{alltt}
\usepackage{wasysym}
\usepackage{numprint}

% Use default Acrobat reader fonts
\usepackage{mathpazo}

% Use CM fonts (increases document size)
\usepackage{ae}

% Use images
\usepackage{graphicx}

% Configure beamer
\usetheme[secheader]{Ikhono}
\usefonttheme[onlylarge]{structurebold}
\setbeamertemplate{navigation symbols}{}

% Variables
\providecommand{\Title}{An Advanced Scheduler for Intervals}
\providecommand{\Subtitle}{Master's Thesis}
\providecommand{\Author}{Thomas Weibel <weibelt@ethz.ch>}
\providecommand{\Institute}{Laboratory for Software Technology, \\
  Swiss Federal Institute of Technology Z\"urich}
\providecommand{\Date}{September 7, 2010}

% PDF settings
\hypersetup{
  pdftitle={\Title, \Subtitle},
  pdfauthor={\Author},
  pdfsubject={\Institute},
  pdfkeywords={Master's Thesis, Thomas Weibel, Intervals, Parallel Programming}
}

% Titlepage
\title{\Title}
\subtitle{\Subtitle}
\author{\Author}
\institute{\Institute}
\date{\Date}

% Listings
\lstdefinestyle{Default}{
  language=Java,
  tabsize=2,
  mathescape=true,
  inputencoding=utf8,
  showstringspaces=false,
  fontadjust=true,
  basicstyle=\ttfamily,
  keywordstyle=\color{blue}\bfseries,
}
\lstset{style=Default}


%==============================================================================
% Document
%==============================================================================

\begin{document}


% Titlepage
\begin{frame}[plain]
  \titlepage
\end{frame}

\note{
  \begin{itemize}
  \item Hi and welcome to my talk.
  \item I'm going to present the work from my Master's thesis. The
    topic of my thesis was ``Advanced Scheduling for Intervals''.
  \end{itemize}

  Backup: Intervals are a new, higher-level primitive for
  multi-threaded programming allowing programmers to directly
  construct the program schedule. When using intervals, programmers
  create lightweight tasks and order them using \emph{happens before}
  relations. Programmers need not specify when tasks should block or
  acquire a lock. Instead they define when a task should execute in
  relation to other tasks, and what locks it should hold during
  execution. It is the duty of the runtime system to make this
  schedule pass. They are under active development at ETH ZÃ¼rich as
  part of the PhD research of Niko.
}


\section*{Introduction}

\begin{frame}{Executive Summary}
  \begin{itemize}
  \item Advanced work-stealing scheduler for intervals
    \begin{itemize}
    \item[$\rightarrow$] Locality-aware scheduling using locality
      hints provided by the programmer
    \end{itemize}
  \item Providing locality hints to intervals is optional
    \begin{itemize}
    \item[$\rightarrow$] Performance of locality-ignorant programs
      executed with the new scheduler implementation is comparable to
      the original scheduler
    \end{itemize}
  \item Locality hints improve runtime and cache hit and miss rates
    \begin{itemize}
    \item[$\rightarrow$] \emph{Best locality} placement achieves up to
      $1.15\times$ speedup over \emph{worst} or \emph{ignorant
        locality} placement
    \item[$\rightarrow$] Cache hits are increased by up to $1.5\times$
      and cache misses are reduced by up to $3.1\times$
    \end{itemize}
  \end{itemize}
\end{frame}

\note{ 
  \begin{itemize}
  \item In my thesis we implemented and analyzed an advanced scheduler
    for intervals which is designed for locality-aware scheduling
    using locality hints provided by the programmer.
  \item Providing locality hints to intervals is optional and the
    performance of locality-ignorant programs executed with the new
    scheduler implementation is comparable to that of the original
    scheduler.
  \item We studied the performance of our new scheduler implementation
    with benchmarks using data sharing intervals. Our experimental
    results show that \emph{best locality} placement of intervals
    achieves up to 15 percent speedup over \emph{worst} or
    \emph{ignorant locality} placement. Cache hits are increased by up
    to 50 percent and cache misses are reduced by up to 310 percent.
  \end{itemize}
}

\begin{frame}{Work-Stealing Intervals Scheduler}
  \begin{itemize}
  \item Employs a fixed number of worker threads
  \item Each worker has a local deque to maintain its own pool of
    ready intervals:
    \begin{itemize}
    \item Puts and takes intervals to execute at the tail of its deque
    \item When its deque is empty, tries to steal an interval from the
      deque's head of a victim worker chosen at random
    \end{itemize}
  \item Reduces contention by having owner and thieves working on
    opposite sides of the deque
    \begin{itemize}
    \item Owner manipulates its deque in a LIFO (stack-like) manner
    \item Stealing operates in a FIFO manner
    \end{itemize}
  \end{itemize}
\end{frame}

\note{
  \begin{itemize}
  \item The implementation of intervals makes use of a work-stealing
    scheduler similar to those found in Cilk or Java 7.
  \item The work-stealing intervals scheduler employs a fixed number
    of threads called workers.
  \item Each worker has a local double-ended queue to maintain its own
    pool of ready intervals:
    \begin{itemize}
    \item When assigning a new interval to a worker, the worker puts
      it onto the tail of its deque. And when obtaining work, it takes
      a ready interval from the tail of its deque to execute.
    \item When a worker's deque is empty, it tries to steal an
      interval from the deque's head of a victim worker chosen at
      random. If the victim's deque is empty, then the thief picks
      another victim worker until it finds a victim whose deque it not
      empty.
    \end{itemize}
  \item Work-stealing scheduling reduces contention by having owner
    and thieves working on opposite sides of the deque: 
    \begin{itemize}
    \item As long as a worker's deque is not empty, the worker
      manipulates its deque in a stack-like manner.
    \item And since steals take place at the head of the victim's
      deque, stealing operates in a FIFO manner.
    \end{itemize}
  \end{itemize}
}

\begin{frame}{Locality-Aware Intervals Scheduling}
  \begin{itemize}
  \item Modern CMPs feature heterogeneous memory hierarchy:
    \begin{itemize}
    \item Access times depend on which processor an interval is
      running
    \item May be more efficient to run an interval on one processor
      than another
    \end{itemize}
  \item Locality-aware intervals can lead to improved performance:
    \begin{itemize}
    \item Data sharing intervals running on the same processor perform
      prefetching of shared regions for one another
    \item Running non-communicating intervals with high memory
      footprints on different processors reduces cache contention
    \end{itemize}
  \item Current work-stealing intervals scheduler is locality-ignorant
  \end{itemize}

  \begin{itemize}
  \item[$\Rightarrow$] Introduce LASSI\footnote{The correct acronym
      would be LASI but we chose LASSI instead as we really enjoy
      drinking refreshing masala lassi \smiley}, a locality-aware
    scheduler for intervals
  \end{itemize}
\end{frame}

\note{
  \begin{itemize}
  \item As modern chip multiprocessor systems feature a heterogeneous
    memory hierarchy where access times depend on which processor an
    interval is running, it may be more efficient to schedule an
    interval on one processor than another.
  \item Locality-aware intervals can lead to improved performance:
    \begin{itemize}
    \item By scheduling data sharing intervals on the same processor
      they perform prefetching of shared regions for one another.
    \item Scheduling non-communicating intervals with high memory
      footprints on different processors helps to reduce cache
      contention and potential cache capacity problems.
    \end{itemize}
  \item The current implementation of the intervals library uses a
    locality-ignorant work-stealing scheduler to schedule ready-to-run
    intervals.
  \item When using locality-ignorant work-stealing we cannot fully
    exploit the heterogeneous memory hierarchy of CMPs for our
    benefit. Thus, we implement and analyze LASSI, a locality-aware
    scheduler for intervals.
  \end{itemize}
}


\section{Approach}

\begin{frame}{Outline}
  \tableofcontents[current]
\end{frame}

\note{
  \begin{itemize}
  \item Before starting with the implementation of the new scheduler,
    we implement a synthetic multi-threaded locality-aware benchmark
    called \emph{Cache Stress Test}.
  \item This benchmark serves as a proof of concept for our plan to
    introduce locality-aware intervals:
    \begin{itemize}
    \item If a multi-threaded benchmark with best possible locality
      has better performance and fewer last-level cache misses than
      the same benchmark with another or no specific locality, we
      should be able to see the same effect when porting the benchmark
      to a locality-aware implementation of intervals.  
    \item Hence, we know whether it makes sense to design a
      locality-aware intervals scheduler.
    \end{itemize}
  \end{itemize}
}

\begin{frame}{Cache Stress Test}
  \begin{columns}[c]
    \begin{column}{0.47\textwidth}
      \begin{itemize}
      \item Randomly initializes two integer arrays of size 8 MB
      \item Binds 8 \emph{Cache Stress} threads to each core
      \item Half of the threads work with array 0, the other half with
        array 1
      \item Each thread adds and multiplies all the elements of its
        array 100 times
      \end{itemize}
    \end{column}
    \begin{column}{0.53\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{figures/mafushi} \\
        \tiny{Intel Nehalem Test System}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\note{
  \begin{itemize}
  \item We wrote our benchmark for ``mafushi'', our Intel Nehalem
    system. ``mafushi'' system has 2 processors with 4 cores
    each. Every core has its separate level 1 and 2 caches, but the
    per-processor 8 MB level 3 cache is shared between all cores of
    the same processor.
  \item \emph{Cache Stress Test} first randomly initializes two
    integer arrays of size 8 MB. This is equal to the size of the last
    level cache per processor.
  \item Then the benchmark creates 8 \emph{Cache Stress} threads per
    core with their affinity set to this specific core. Overall, we
    create 64 threads, bound to the 8 cores in groups of 8 threads.
  \item One half of the threads operate on the elements of the first
    array and the other half operate on the elements of the second
    array.
  \item Each thread adds and multiplies all the elements of its
    respective array 100 times.
  \end{itemize}
}

\begin{frame}{Best and Worst Locality}
  \begin{columns}[c]
    \begin{column}{0.50\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{figures/cache-stress-test-mafushi-best} \\
        \tiny{Best Locality}     
      \end{center}
    \end{column}
    \begin{column}{0.50\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{figures/cache-stress-test-mafushi-worst} \\
        \tiny{Worst Locality}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\note{
  We implement several different variants of the \emph{Cache Stress
    Test}, each having different locality properties. The figure
  illustrates \emph{Best} and \emph{Worst Locality}:

  \begin{itemize}
  \item In the \emph{Best Locality} implementation all the threads
    working on the first array have affinity for a core on the first
    processor and all threads working on the second array have
    affinity for a core on the second processor.
  \item When using \emph{Worst Locality}, half the threads with
    affinity for a core on the first processor work on the first
    array, and the other half works on the second array and vice
    versa.
  \end{itemize}
  
  Besides \emph{Best} and \emph{Worst Locality} we also implement
  variants with \emph{Ignorant} and \emph{Random Locality}:

  \begin{itemize}
  \item In the \emph{Ignorant Locality} variant, the threads are not
    bound to any specific core, i.e. they are \emph{ignorant} of their
    locality.
  \item When using \emph{Random Locality}, the affinity of the threads
    is set to a \emph{random} core.
  \end{itemize}
}

\begin{frame}{Execution Times}
  \begin{columns}[c]
    \begin{column}{0.49\textwidth}
      \begin{itemize}
      \item \emph{Best locality} variant: Sharing threads run on same
        processor, perform prefetching of array elements for each
        other
      \item Other variants: Threads compete for L3 caches, overwriting
        each other's entries
      \item \emph{Best locality} has significant speedup of up to
        $1.55\times$
  \end{itemize}
    \end{column}
    \begin{column}{0.51\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{figures/cache-stress-test} \\
        \tiny{Execution times normalized to \emph{best locality}
          implementation}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\note{
  \begin{itemize}
  \item As the name already gives away, the main idea behind the
    \emph{Cache Stress Test} benchmark is to stress the cache and
    provoke cache prefetching and contention.
  \item When we are using the \emph{best locality} variant, we move
    all sharing threads onto the same processor which will perform
    prefetching of the array elements for each other. That is, they
    help to obtain and maintain the frequently used array elements in
    the local L3 cache.
  \item The exact opposite happens in the other variants: Threads
    compete for the L3 caches and overwrite each other's
    entries. Instead of reading from the processor's local cache,
    threads either have to read from the other processor's cache or
    the memory subsystem. Reading from those places is much more
    expensive than reading from the local L3 cache.
  \item The \emph{best locality} implementation shows a significant
    speedup over the other locality benchmarks of $1.2\times$ --
    $1.55\times$.
  \end{itemize}
}

\begin{frame}{Cache Read Hits and Misses}
  \begin{itemize}
  \item \emph{Best locality} benchmark has between $1.5\times$ and
    $1.8\times$ more L3 cache read hits, and between $3.6\times$ and
    $4.5\times$ fewer read misses
  \item Cache read hits and misses normalized to the \emph{best
      locality} implementation:
  \end{itemize}
  
  \vspace{\stretch{1}}

  \begin{columns}[c]
    \begin{column}{0.50\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{figures/cache-stress-test-cache-hits} \\
        \tiny{Cache Read Hits}     
      \end{center}
    \end{column}
    \begin{column}{0.50\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{figures/cache-stress-test-cache-misses} \\
        \tiny{Cache Read Misses}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\note{
  \begin{itemize}
  \item Compared to the other benchmarks, the \emph{best locality}
    benchmark has between $1.5\times$ and $1.8\times$ more L3 cache
    read hits, and between $3.6\times$ and $4.5\times$ fewer L3 cache
    read misses.
  \item In the figure they are shown normalized to the measurements of
    the \emph{best locality} implementation.
  \end{itemize}
}


\section{Implementation}

\begin{frame}{Outline}
  \tableofcontents[current]
\end{frame}

\note{
  \begin{itemize}
  \item Our experiments confirmed the hypothesis that the locality of
    threads matters.
  \item Thus, we decided to rewrite the intervals scheduler to support
    locality hints provided by the programmer.
  \end{itemize}
}

\subsection{Locality-Aware Intervals}

\begin{frame}[fragile]{Locality-Aware Intervals API}
  \begin{itemize}
  \item Intervals are subtypes of abstract class \lstinline!Interval!
  \item Specify the interval's locality when creating it
  \item Locality hints provided in the form of \lstinline!PlaceID!
    objects
    \begin{itemize}
    \item[$\rightarrow$] Assign the interval to the specified place
    \end{itemize}
  \item If \lstinline!PlaceID! is null, the interval is ignorant of
    its place    
    \begin{itemize}
    \item[$\rightarrow$] Assign the interval to a place in a
      round-robin fashion
    \end{itemize}
  \end{itemize}

  \vspace{\stretch{1}}

\begin{lstlisting}[basicstyle=\fontsize{8}{10}\selectfont\ttfamily]
  public abstract class Interval extends WorkItem {	
    public final PlaceID place;

    public Interval(Dependency dep, String name, PlaceID place) {
      this.place = place;  
      // ...
    }
    
    // ...
  }\end{lstlisting}
\end{frame}

\note{
  \begin{itemize}
  \item Intervals are represented as subtypes of the abstract class
    \lstinline!Interval!.
  \item To make an interval locality-aware, the programmer has to
    specify the interval's locality when creating it. We extend the
    abstract \lstinline!Interval! class to support locality hints.
  \item Locality hints are provided in the form of \lstinline!PlaceID!
    objects. They specify which place the interval should be executed
    on.
  \item If the interval should be ignorant of its place, the
    programmer can provide \lstinline!null! when creating it. This
    makes the scheduler to assign the interval to a place in a
    round-robin fashion.
  \end{itemize}
}

\subsection{Work-Stealing Places}

\begin{frame}{Work-Stealing Places}
  \begin{itemize}
  \item Work-stealing scheduler has fixed number of worker threads
  \item Traditional work-stealing scheduler designs: Every worker has
    local deque to maintain own pool of ready tasks
  \item LASSI uses \emph{Work-Stealing Places} instead:
    \begin{itemize}
    \item Each place has a fixed number of workers and a local deque
      to maintain ready tasks
    \item Workers of a place share its local deque
    \item When the pool of a place is empty, its workers tries to
      steal a task from the pool of a victim place chosen at random
    \end{itemize}
  \item Number of places fixed at the launch of an intervals program
  \end{itemize}
\end{frame}

\note{
  \begin{itemize}
  \item A work-stealing scheduler employs a fixed number of threads
    called workers.
  \item In traditional work-stealing scheduler designs, every worker
    has a local double-ended queue, or deque, to maintain its own pool
    of ready tasks from which it obtains work.
  \item LASSI uses \emph{Work-Stealing Places} instead:
    \begin{itemize}
    \item Each work-stealing place has a fixed number of workers and a
      local deque to maintain ready tasks.
    \item The workers of a place share its local deque from which they
      obtain work.
    \item When a worker finds that the pool of its place is empty, it
      tries to steal a task from the pool of a victim place chosen at
      random.
    \end{itemize}
  \item The number of places is fixed at the time an intervals program
    is launched: There is no construct to create a new place
  \end{itemize}
}

\begin{frame}{Intel Nehalem in Two-Processor Configuration}
  \begin{center}
    \includegraphics[width=0.93\textwidth]{figures/places} \\
    \tiny{\emph{Work-Stealing Places} used in our Intel Nehalem
      testing machine}
  \end{center}
\end{frame}

\note{
  \begin{itemize}
  \item The figure shows the work-stealing places used on our Intel
    Nehalem testing machine.
  \item Places are virtual: The mapping of physical units to places is
    performed by a concrete implementation of the abstract
    \lstinline!Places! class.
  \item For our test machine we define a place for each processor:
    Place 0 consists of the physical units 0, 2, 4, and 6 and the
    physical units 1, 3, 5, and 7 form place 1.
  \end{itemize}
}

\begin{frame}{Alternative Designs}
  TODO
\end{frame}

\note{
  \begin{itemize}
  \item Other locality-aware work-stealing schedulers use a more
    complex design. Acar et al. for example provide each worker with a
    mailbox in addition to the work-stealing deque: ``A mailbox is a
    FIFO queue of pointers to work items that have affinity for the
    worker. So when creating a work item, a worker will push it onto
    both the deque, as in normal work-stealing, and also onto the tail
    of the mailbox of the worker that the interval has affinity
    for. Now a worker will first try to obtain work from its mailbox
    before attempting to steal. Because work items can appear twice,
    once in a mailbox and once in a deque, they have to be
    idempotent.''
  \item However, we have decided to simplify our scheduler
    implementation by using a shared deque per \emph{Work-Stealing
      Place}. We believe that this would not impact scalability as
    long as the places are not too large. We could show that up to 8
    workers there is no significant difference between using a
    separate deque for each worker or a shared deque per place.
  \end{itemize}
}

\begin{frame}{Setting Core Affinity of Worker Threads}
  \begin{columns}[c]
    \begin{column}{0.65\textwidth}
      \begin{itemize}
      \item TODO
      \end{itemize}
    \end{column}
    \begin{column}{0.35\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{figures/java-threads} \\
        \tiny{Linux 1-to-1 thread mapping}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\note{
  \begin{itemize}
  \item In recent Java Virtual Machines, threads are implemented with
    native threads. A Java program using threads is no different from
    a native program using threads, i.e. a Java thread is just a
    native thread belonging to a JVM process. This means there is a
    1-to-1 correspondence between Java and native threads. When using
    the GNU C library on Linux, native threads are implemented with
    the NPTL (Native POSIX Threads Library). NPTL is also a 1-to-1
    implementation, meaning that each thread maps to a kernel
    scheduling entity.
  \item Unfortunately the Java Threads API does not expose the ability
    to set the CPU or core affinity despite numerous use cases where
    setting the affinity of threads would be beneficial -- such as
    improving cache and network performance or real-time
    applications. There exists a request for enhancement on this issue
    but it was set to the state \emph{``Closed, Will Not Fix''}.
  \item To bind the workers to a specific core, we wrote a small JNI
    library:
    \begin{itemize}
    \item With \lstinline!pthread_self()! we get the native thread
      ID.
    \item \lstinline!pthread_setaffinity_np()! sets the core affinity
      of the worker thread.
    \end{itemize}
  \end{itemize}
}

\begin{frame}{Restrictions}
  TODO
\end{frame}

\note{
  \begin{itemize}
  \item Portability:
    \begin{itemize}
    \item As we are directly using functions provided by POSIX
      threads, our implementation is not portable across operating
      systems that do not support the POSIX standard. To make our
      library portable, it could be rewritten using \emph{Portable
        Linux Processor Affinity (PLPA)} or \emph{Portable Hardware
        Locality (hwloc)}.
    \end{itemize}
  \item Data Locality:
    \begin{itemize}
    \item By setting the core affinity of threads, we only control the
      locality of the work but we do not have control over data
      locality.
    \item In a NUMA system every node has a direct connection to local
      memory providing fast access, and an indirect connection to
      remote memory with slower access. In the Intel Nehalem system
      for example, local DRAM access takes $\sim 60$ ns, while remote
      DRAM access takes $\sim 100$ ns.
    \item In the Java HotSpot VM, the NUMA-aware allocator has been
      implemented to provide automatic memory placement optimizations
      for Java applications: ``The allocator controls the eden space
      of the young generation of the heap, where most of the new
      objects are created. It divides the space into regions each of
      which is placed in the memory of a specific node. The allocator
      relies on a hypothesis that a thread that allocates the object
      will be the most likely to use the object. To ensure the fastest
      access to the new object, the allocator places it in the region
      local to the allocating thread.''
    \item To enable the NUMA-aware allocator, we invoke the JVM with
      the parameter \lstinline!-XX:+UseNUMA! when using LASSI.
    \end{itemize}
  \end{itemize}
}


\section{Performance Evaluation}

\begin{frame}{Outline}
  \tableofcontents[current]
\end{frame}

\note{
  TODO
}

\begin{frame}{TODO}
  TODO
\end{frame}

\note{
  TODO
}


\section{Conclusions and Future Work}

\begin{frame}{Outline}
  \tableofcontents[current]
\end{frame}

\note{
  TODO
}

\begin{frame}{Conclusions}
  \begin{itemize}
  \item Introduced LASSI, a locality-aware scheduler for intervals
    using locality hints provided by the programmer
  \item \emph{Work-Stealing Places} to support locality-awareness
  \item Performance of existing locality-ignorant programs comparable
    to the original scheduler implementation
  \item Scheduling data sharing intervals on the same processor:
    \begin{itemize}
    \item[$\rightarrow$] Prefetching of shared regions for one another
    \item[$\rightarrow$] Up to $1.15\times$ speedup over
      \emph{ignorant locality} placement
    \item[$\rightarrow$] Cache hits increased by up to $1.5\times$ and
      cache misses reduced by up to $3.1\times$
    \end{itemize}
  \item Benchmarks do not test scheduling non-communicating intervals
    with high memory footprints on different processors
  \item Difficult to write benchmarks exploiting the memory hierarchy
    of NUMA systems
  \end{itemize}
\end{frame}

\note{
  \begin{itemize}
  \item In this thesis, we introduced LASSI, a locality-aware
    scheduler for intervals using locality hints provided by the
    programmer.
  \item Instead of employing work-stealing workers, LASSI uses
    \emph{Work-Stealing Places} to provide locality-awareness.
  \item The performance of existing locality-ignorant programs run
    with LASSI is comparable to the original scheduler implementation.
  \item By scheduling data sharing intervals on the same processor
    they perform prefetching of shared regions for one another which
    improves performance. Our experimental results show that
    \emph{best locality} placement of intervals can achieve up to 15
    percent speedup over locality-ignorance. Cache hits can be
    increased by up to 50 percent and cache misses can be reduced by
    up to 310 percent.
  \item Our benchmarks do not test how scheduling non-communicating
    intervals with high memory footprints on different processors
    helps to reduce cache contention and potential cache capacity
    problems.
  \item In general, it is not easy to write benchmarks exploiting the
    memory hierarchy of NUMA systems. Many benchmarks mentioned in the
    literature do not show the desired effects anymore due to advances
    in hardware and software.
  \end{itemize}
}

\begin{frame}{Future Work}
  \begin{itemize}
  \item Improve API of \emph{Work-Stealing Places} and locality-aware
    intervals
  \item Make underlying machine transparent to the user
  \item Extend \emph{Work-Stealing Places} to co-locate tasks and
    data
  \item Avoid counter-productive steals when load balancing across
    work-stealing places
  \item Online contention detection to dynamically reduce or increase
    the number of worker threads depending on the system load
  \item Explore \emph{Parallel Depth-First Scheduling} as a possible
    replacement for \emph{Work-Stealing Scheduling} in intervals
  \end{itemize}
\end{frame}

\note{
  \begin{itemize}
  \item A possible area of future work would be to improve the API of
    \emph{Work-Stealing Places} and locality-aware intervals.
  \item Places have to be manually configured for each system. This
    should be automated by making the hardware transparent to the
    user.
  \item LASSI depends on the heuristics of the NUMA-aware allocator
    implemented in Java HotSpot VM to provide automatic memory
    placement optimizations. Further research could be done in
    extending \emph{Work-Stealing Places} to co-locate tasks and data.
  \item Load balancing across work-stealing places can lead to
    counter-productive stealing. Future work should avoid such steals.
  \item Worker threads share their assigned core with other processes
    in the system. Online contention detection could be used to
    dynamically reduce or increase the number of worker threads.
  \item Another possible direction for future research would be to
    explore \emph{Parallel Depth-First Scheduling} as a possible
    replacement for \emph{Work-Stealing Scheduling} in intervals. In
    PDF scheduling, tasks are assigned priorities in the same ordering
    they would be executed in a sequential program. As a result, PDF
    scheduling tends to employ constructive cache sharing as it
    co-schedules threads in a way that simulates the sequential
    execution order.
  \end{itemize}
}


\section*{Outro}

\begin{frame}
  \begin{center}
    \huge Questions?
  \end{center}
\end{frame}

\note{
  TODO
}


\appendix

\section{Appendix}

\subsection{Bibliography}

\begin{frame}
  \begin{thebibliography}{10}
    % Articles
    \beamertemplatearticlebibitems

  \bibitem{data-locality} Umut A. Acar et al. {\em``The data locality
      of work stealing''}. 2000.

  \bibitem{ws-sched} Robert D. Blumofe and Charles
    E. Leiserson. {\em``Scheduling multithreaded computations by work
      stealing''}. 1999.

  \bibitem{cilk} Robert D. Blumofe et al. {\em``Cilk: an efficient
      multithreaded runtime system''}. 1995.

  \bibitem{slaw} Yi Guo et al. {\em``SLAW: a scalable locality-aware
      adaptive work-stealing scheduler for multi-core
      systems''}. 2010.

  \bibitem{java-fork-join} Doug Lea. {\em``A Java fork/join
      framework''}. 2000.

  \bibitem{intervals} Nicholas D. Matsakis and Thomas
    R. Gross. {\em``Programming with Intervals''}. 2009.

  \bibitem{cache-affinity} M. S. Squillante and
    E. D. Lazowska. {\em``Using Processor-Cache Affinity Information
      in Shared-Memory Multiprocessor Scheduling ``}. 1993.

  \bibitem{cmp} Bratin Saha et al. {\em``Enabling scalability and
      performance in a large scale CMP environment''}. 2007.

  \end{thebibliography}
\end{frame}

\note{
  Those are a few selected references I used in my work.
}


\subsection{Work-Stealing Queue Implementations}

\begin{frame}{Alternative Work-Stealing Queues}
  \begin{itemize}
  \item Performance of work-stealing schedulers in large part
    determined by the efficiency of the work queue
  \item Non-blocking work-stealing scheduler queues employ atomic
    synchronization primitives such as Compare-and-Swap instead of
    using mutual exclusion
  \item Current work-stealing queue of intervals uses mutual exclusion
    when trying to steal
  \end{itemize}

  \vspace{\stretch{1}}

  \begin{itemize}
  \item[$\Rightarrow$] As a separate effort, we design and explore
    alternative non-blocking queue implementations with the aim to
    improve work-stealing performance
  \end{itemize}
\end{frame}

\note{
  \begin{itemize}
  \item The performance of work-stealing schedulers is in a large part
    determined by the efficiency of their work queue
    implementations.
  \item In the non-blocking work-stealing scheduler, the
    queues are implemented with non-blocking synchronization. That is,
    instead of using mutual exclusion, it uses atomic synchronization
    primitives such as Compare-and-Swap.
  \item The current work-stealing queue of intervals however uses
    mutual exclusion when trying to steal.
  \item Thus, as a separate effort, we design and explore alternative
    non-blocking queue implementations with the aim to improve
    work-stealing performance.
  \end{itemize}
}

\begin{frame}{Results}
  \begin{itemize}
  \item Hypothesis was that we could improve the performance of the
    intervals scheduler with non-blocking work-stealing queues
  \item Evaluated the performance of our queue implementations with
    intervals implementations of various Java Grande Forum benchmarks
  \item None of the alternative work-stealing queues significantly
    improves performance on the machines we had to test
  \end{itemize}

  \vspace{\stretch{1}}

  \begin{block}{Possible Reason}
    There is no noticeable difference between the speedup of
    work-stealing and a global shared work queue when not using more
    than 8 cores
  \end{block}
\end{frame}

\note{
  \begin{itemize}
  \item Our hypothesis was that we could improve the performance of
    the intervals scheduler with non-blocking work-stealing queues.
  \item We evaluated the performance of our alternative queues by
    using the intervals implementations of various Java Grande Forum
    benchmarks.
  \item None of the work-stealing queues we developed significantly
    improves work-stealing performance on the machines we had to test
    them with.
  \item Apart from the more complex implementation in comparison to
    the original \emph{Work-Stealing Deque}, a possible reason for
    this could be the rather small number of cores of our benchmark
    machines. We could not find any significant difference between the
    runtimes of the Java Grande Forum benchmarks when using the
    different implementations -- whether we use work-stealing or a
    single shared work queue, we get almost the same results.
  \end{itemize}
}

\begin{frame}{Future Work}
  \begin{itemize}
  \item Explore the performance of the different work-stealing queue
    implementations on machines with more than 8 cores
  \item See how our work-stealing queues would benefit from using the
    steal-half algorithm of Hendler and Shavit
  \item Prevent waste of memory by shrinking queues using arrays to
    maintain work items again once they contain less items
  \item Using a shared pool of arrays:
    \begin{itemize}
    \item When the queue needs a larger array, it allocates one of the
      appropriate size from the pool
    \item Whenever it shrinks to a smaller array and does not need the
      larger array anymore, it can return it to the pool
    \end{itemize}
  \end{itemize}
\end{frame}

\note{
  \begin{itemize}
  \item Given the results of the performance evaluation, a direction
    for future research would be to explore the performance of the
    different work-stealing queue implementations on machines
    featuring more than 8 cores.
  \item It may also be interesting to see how our work-stealing queues
    would benefit from using the steal-half algorithm of Hendler and
    Shavit.
  \item One disadvantage of the queues using arrays to maintain work
    items is that they do not shrink them again once the queue
    contains less work items which might lead to a waste of
    memory. Chase and Lev present a way of shrinking an array without
    copying by keeping the reference to the smaller array when
    expanding. When the queue shrinks back its array to the previous
    array, only the elements that were modified while the larger array
    was active need to be copied.
  \item Another optimization Chase and Lev mention is working with a
    shared pool of arrays. With the shared pool implementation,
    whenever the queue needs a larger array, it allocates one of the
    appropriate size from the pool, and whenever it shrinks to a
    smaller array and does not need the larger array anymore, it can
    return it to the pool.
  \end{itemize}
}

\end{document}
